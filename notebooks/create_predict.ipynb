{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alshouha\\.python_venvs\\transformers\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import pickle as pkl\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "from datasets import load_from_disk, Dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train = train_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(tokenizer, examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=9)\n",
    "\n",
    "def translate_text(str, src=\"en\", dest=\"eo\"):\n",
    "    translator = Translator()\n",
    "    return translator.translate(str, src=src, dest=dest).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD (55 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multi', 'xlm', 'xln']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\n",
    "    \"multi\" :   dict(),\n",
    "    \"xlm\"   :   dict(),\n",
    "    \"xln\"   :   dict()\n",
    "    }\n",
    "\n",
    "model_names = list(models.keys())\n",
    "\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alshouha\\.python_venvs\\transformers\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "models_dir = \"./classifiers/ood_finetuned\"\n",
    "for m in model_names:\n",
    "  tokenizer_link = f\"{models_dir}/{m}/{m}_tokenizer\"\n",
    "  model_link = f\"{models_dir}/{m}/{m}_model\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(tokenizer_link)\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(model_link)\n",
    "  classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, function_to_apply=\"none\", return_all_scores=True)\n",
    "  models[m][\"tokenizer\"] = tokenizer\n",
    "  models[m][\"model\"] = model\n",
    "  models[m][\"classifier\"] = classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117674935"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi     \t: 117674935\n",
      "xlm     \t: 278085943\n",
      "xln     \t: 117351223\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "  model = models[model_name][\"model\"]\n",
    "  print(f\"{model_name}     \\t: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Massive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASSIVE english test\n",
    "en_dataset = load_from_disk(\"datasets\\massive_en_test.hf\")\n",
    "\n",
    "# # Esperanto english test\n",
    "# es_dataset = load_from_disk(\"datasets\\esperanto.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "categroies_list = [\n",
    "    'datetime_query',\n",
    "    'iot_hue_lightchange',\n",
    "    'transport_ticket',\n",
    "    'takeaway_query',\n",
    "    'qa_stock',\n",
    "    'general_greet',\n",
    "    'recommendation_events',\n",
    "    'music_dislikeness',\n",
    "    'iot_wemo_off',\n",
    "    'cooking_recipe',\n",
    "    'qa_currency',\n",
    "    'transport_traffic',\n",
    "    'general_quirky',\n",
    "    'weather_query',\n",
    "    'audio_volume_up',\n",
    "    'email_addcontact',\n",
    "    'takeaway_order',\n",
    "    'email_querycontact',\n",
    "    'iot_hue_lightup',\n",
    "    'recommendation_locations',\n",
    "    'play_audiobook',\n",
    "    'lists_createoradd',\n",
    "    'news_query',\n",
    "    'alarm_query',\n",
    "    'iot_wemo_on',\n",
    "    'general_joke',\n",
    "    'qa_definition',\n",
    "    'social_query',\n",
    "    'music_settings',\n",
    "    'audio_volume_other',\n",
    "    'calendar_remove',\n",
    "    'iot_hue_lightdim',\n",
    "    'calendar_query',\n",
    "    'email_sendemail',\n",
    "    'iot_cleaning',\n",
    "    'audio_volume_down',\n",
    "    'play_radio',\n",
    "    'cooking_query',\n",
    "    'datetime_convert',\n",
    "    'qa_maths',\n",
    "    'iot_hue_lightoff',\n",
    "    'iot_hue_lighton',\n",
    "    'transport_query',\n",
    "    'music_likeness',\n",
    "    'email_query',\n",
    "    'play_music',\n",
    "    'audio_volume_mute',\n",
    "    'social_post',\n",
    "    'alarm_set',\n",
    "    'qa_factoid',\n",
    "    'calendar_set',\n",
    "    'play_game',\n",
    "    'alarm_remove',\n",
    "    'lists_remove',\n",
    "    'transport_taxi',\n",
    "    'recommendation_movies',\n",
    "    'iot_coffee',\n",
    "    'music_query',\n",
    "    'play_podcasts',\n",
    "    'lists_query']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "        52, 53, 54, 55, 56, 57, 58, 59]),\n",
       " array([ 88,  36,  35,  35,  26,   1,  43,   4,  18,  72,  39,  15, 169,\n",
       "        156,  13,  12,  22,  26,  27,  31,  41,  39, 124,  34,  10,  19,\n",
       "         57,  25,   6,   6,  67,  21, 126, 114,  26,  11,  72,  15,  25,\n",
       "         43,   3,  51,  36, 119, 176,  32,  81,  41, 141, 209,  35,  21,\n",
       "         52,  23,  20,  36,  35,  63,  51], dtype=int64))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list, counts = np.unique(en_dataset[\"intent\"], return_counts=True)\n",
    "labels_list, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.06893073301950235)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ood = 5\n",
    "len(counts[-num_ood:]), np.sum(counts[-num_ood:])/np.sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ood_labels = set(labels_list[-num_ood:])\n",
    "\n",
    "num_labels = len(categroies_list)-len(ood_labels)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2974, 2974)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_X_test = np.array(en_dataset[\"utt\"])\n",
    "en_Y_test = np.array(en_dataset[\"intent\"])\n",
    "\n",
    "len(en_X_test), len(en_Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ID = np.ones(en_X_test.shape)\n",
    "for i in range(len(en_Y_test)):\n",
    "  if en_Y_test[i] in ood_labels: test_ID[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2769, 2769, 205, 205)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_X_test_ID = en_X_test[np.where(test_ID==1)]\n",
    "en_Y_test_ID = en_Y_test[np.where(test_ID==1)]\n",
    "\n",
    "en_X_test_OOD = en_X_test[np.where(test_ID==0)]\n",
    "en_Y_test_OOD = en_Y_test[np.where(test_ID==0)]\n",
    "\n",
    "len(en_X_test_ID), len(en_Y_test_ID), len(en_X_test_OOD), len(en_Y_test_OOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_X_test_ID) + len(en_X_test_OOD) == len(en_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2974-2769 == np.sum(counts[-num_ood:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ID = {\"label\":en_Y_test_ID.copy(), \"text\":en_X_test_ID.copy()}\n",
    "en_ID = Dataset.from_pandas(pd.DataFrame(data=en_ID))\n",
    "\n",
    "en_OOD = {\"label\":en_Y_test_OOD.copy(), \"text\":en_X_test_OOD.copy()}\n",
    "en_OOD = Dataset.from_pandas(pd.DataFrame(data=en_OOD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2769/2769 [00:00<00:00, 236817.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 205/205 [00:00<00:00, 68322.00 examples/s] \n"
     ]
    }
   ],
   "source": [
    "en_ID.save_to_disk(\"./datasets/en_ID_test.hf\")\n",
    "en_OOD.save_to_disk(\"./datasets/en_OOD_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_query = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2435, 2435)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in en_ID:\n",
    "  query = item[\"text\"]\n",
    "  if len(query.split())<=short_query:\n",
    "    texts.append(query)\n",
    "    labels.append(item[\"label\"])\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([48, 46, 1, 41, 40],\n",
       " ['wake me up at five am this week',\n",
       "  'quiet',\n",
       "  'pink is all we need',\n",
       "  'and the darkness has fallen',\n",
       "  'olly turn the lights off in the bedroom'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2435/2435 [00:00<00:00, 116978.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "en_ID_short = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "en_ID_short = Dataset.from_pandas(pd.DataFrame(data=en_ID_short))\n",
    "en_ID_short.save_to_disk(\"./datasets/en_ID_short_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 190)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in en_OOD:\n",
    "  query = item[\"text\"]\n",
    "  if len(query.split())<=short_query:\n",
    "    texts.append(query)\n",
    "    labels.append(item[\"label\"])\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([56, 57, 57, 57, 56],\n",
       " ['run coffee maker',\n",
       "  'who is the singer of song',\n",
       "  'i want to know about this song',\n",
       "  'who sings the song about a long black train',\n",
       "  'i want a coffee'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 190/190 [00:00<00:00, 19700.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "en_OOD_short = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "en_OOD_short = Dataset.from_pandas(pd.DataFrame(data=en_OOD_short))\n",
    "en_OOD_short.save_to_disk(\"./datasets/en_OOD_short_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_query = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2435, 2435)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in en_ID_short:\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  query_len = len(query.split())\n",
    "  repeat = math.ceil(long_query/query_len)\n",
    "  query = [query for _ in range(repeat)]\n",
    "  query = \", \".join(query)\n",
    "  texts.append(query)\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([48, 46, 1, 41, 40],\n",
       " ['wake me up at five am this week, wake me up at five am this week, wake me up at five am this week, wake me up at five am this week',\n",
       "  'quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet, quiet',\n",
       "  'pink is all we need, pink is all we need, pink is all we need, pink is all we need, pink is all we need, pink is all we need',\n",
       "  'and the darkness has fallen, and the darkness has fallen, and the darkness has fallen, and the darkness has fallen, and the darkness has fallen, and the darkness has fallen',\n",
       "  'olly turn the lights off in the bedroom, olly turn the lights off in the bedroom, olly turn the lights off in the bedroom, olly turn the lights off in the bedroom'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2435/2435 [00:00<00:00, 155804.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "en_ID_long = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "en_ID_long = Dataset.from_pandas(pd.DataFrame(data=en_ID_long))\n",
    "en_ID_long.save_to_disk(\"./datasets/en_ID_long_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 190)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in en_OOD_short:\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  query_len = len(query.split())\n",
    "  repeat = math.ceil(long_query/query_len)\n",
    "  query = [query for _ in range(repeat)]\n",
    "  query = \", \".join(query)\n",
    "  texts.append(query)\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([56, 57, 57, 57, 56],\n",
       " ['run coffee maker, run coffee maker, run coffee maker, run coffee maker, run coffee maker, run coffee maker, run coffee maker, run coffee maker, run coffee maker, run coffee maker',\n",
       "  'who is the singer of song, who is the singer of song, who is the singer of song, who is the singer of song, who is the singer of song',\n",
       "  'i want to know about this song, i want to know about this song, i want to know about this song, i want to know about this song, i want to know about this song',\n",
       "  'who sings the song about a long black train, who sings the song about a long black train, who sings the song about a long black train, who sings the song about a long black train',\n",
       "  'i want a coffee, i want a coffee, i want a coffee, i want a coffee, i want a coffee, i want a coffee, i want a coffee, i want a coffee'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 190/190 [00:00<00:00, 12848.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "en_OOD_long = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "en_OOD_long = Dataset.from_pandas(pd.DataFrame(data=en_OOD_long))\n",
    "en_OOD_long.save_to_disk(\"./datasets/en_OOD_long_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esperanto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2769/2769 [08:09<00:00,  5.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2769, 2769)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in tqdm(en_ID):\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  texts.append(translate_text(query))\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([48, 46, 1, 41, 40],\n",
       " ['veku min je la kvina matene ĉi-semajne',\n",
       "  'trankvila',\n",
       "  'rozo estas ĉio, kion ni bezonas',\n",
       "  'kaj la mallumo falis',\n",
       "  'ol estingu la lumojn en la dormoĉambro'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2769/2769 [00:00<00:00, 396938.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "es_ID = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "es_ID = Dataset.from_pandas(pd.DataFrame(data=es_ID))\n",
    "es_ID.save_to_disk(\"./datasets/es_ID_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:36<00:00,  5.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(205, 205)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in tqdm(en_OOD):\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  texts.append(translate_text(query))\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([56, 57, 57, 57, 56],\n",
       " ['kuri kafomaŝinon',\n",
       "  'kiu estas la kantisto de kanto',\n",
       "  'Mi volas scii pri ĉi tiu kanto',\n",
       "  'kiu kantas la kanton pri longa nigra trajno',\n",
       "  'mi volas kafon'])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 205/205 [00:00<00:00, 41207.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "es_OOD = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "es_OOD = Dataset.from_pandas(pd.DataFrame(data=es_OOD))\n",
    "es_OOD.save_to_disk(\"./datasets/es_OOD_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2435/2435 [05:40<00:00,  7.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2435, 2435)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in tqdm(en_ID_short):\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  texts.append(translate_text(query))\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([48, 46, 1, 41, 40],\n",
       " ['veku min je la kvina matene ĉi-semajne',\n",
       "  'trankvila',\n",
       "  'rozo estas ĉio, kion ni bezonas',\n",
       "  'kaj la mallumo falis',\n",
       "  'ol estingu la lumojn en la dormoĉambro'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2435/2435 [00:00<00:00, 487081.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "es_ID_short = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "es_ID_short = Dataset.from_pandas(pd.DataFrame(data=es_ID_short))\n",
    "es_ID_short.save_to_disk(\"./datasets/es_ID_short_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:25<00:00,  7.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(190, 190)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in tqdm(en_OOD_short):\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  texts.append(translate_text(query))\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([56, 57, 57, 57, 56],\n",
       " ['kuri kafomaŝinon',\n",
       "  'kiu estas la kantisto de kanto',\n",
       "  'Mi volas scii pri ĉi tiu kanto',\n",
       "  'kiu kantas la kanton pri longa nigra trajno',\n",
       "  'mi volas kafon'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 190/190 [00:00<00:00, 45894.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "es_OOD_short = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "es_OOD_short = Dataset.from_pandas(pd.DataFrame(data=es_OOD_short))\n",
    "es_OOD_short.save_to_disk(\"./datasets/es_OOD_short_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2435/2435 [09:03<00:00,  4.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2435, 2435)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in tqdm(en_ID_long):\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  texts.append(translate_text(query))\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([48, 46, 1, 41, 40],\n",
       " ['veku min je la kvina matene ĉi-semajne, veku min je la kvina matene ĉi-semajne, veku min je la kvina matene ĉi-semajne, veku min je la kvina matene ĉi-semajne',\n",
       "  'kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, kvieta, trankvila quiet, quiet, quiet, quiet, quiet',\n",
       "  'rozkolora estas ĉio, kion ni bezonas, rozkolora estas ĉio, kion ni bezonas, rozkolora estas ĉio, kion ni bezonas, rozkolora estas ĉio, kion ni bezonas, rozkolora estas ĉio, kion ni bezonas.',\n",
       "  'kaj la mallumo falis, kaj la mallumo falis, kaj la mallumo falis, kaj la mallumo falis, kaj la mallumo falis, kaj la mallumo falis.',\n",
       "  'olly malŝaltu la lumojn en la dormoĉambro, olly malŝaltu la lumojn en la dormoĉambro, olly malŝaltu la lumojn en la dormoĉambro, olly malŝaltu la lumojn en la dormoĉambro'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2435/2435 [00:00<00:00, 405637.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "es_ID_long = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "es_ID_long = Dataset.from_pandas(pd.DataFrame(data=es_ID_long))\n",
    "es_ID_long.save_to_disk(\"./datasets/es_ID_long_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:41<00:00,  4.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(190, 190)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in tqdm(en_OOD_long):\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  texts.append(translate_text(query))\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([56, 57, 57, 57, 56],\n",
       " ['kuri kafomaŝinon, kuri kafmaŝinon, kuri kafomaŝinon, funkciigu kafmaŝinon, kuru kafmaŝinon, kuru kafmaŝinon, kuru kafmaŝinon, kuru kafmaŝinon, kuru kafomaŝinon, kuru kafomaŝinon',\n",
       "  'kiu estas la kantisto de kanto, kiu estas la kantisto de kanto, kiu estas la kantisto de kanto, kiu estas la kantisto de kanto, kiu estas la kantisto de kanto',\n",
       "  'mi volas scii pri ĉi tiu kanto, mi volas scii pri ĉi tiu kanto, mi volas scii pri ĉi tiu kanto, mi volas scii pri ĉi tiu kanto, mi volas scii pri ĉi tiu kanto',\n",
       "  'kiu kantas la kanton pri longa nigra trajno, kiu kantas la kanton pri longa nigra trajno, kiu kantas la kanton pri longa nigra trajno, kiu kantas la kanton pri longa nigra trajno',\n",
       "  'mi volas kafon, mi volas kafon, mi volas kafon, mi volas kafon, mi volas kafon, mi volas kafon, mi volas kafon, mi volas kafon'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 190/190 [00:00<00:00, 31523.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "es_OOD_long = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "es_OOD_long = Dataset.from_pandas(pd.DataFrame(data=es_OOD_long))\n",
    "es_OOD_long.save_to_disk(\"./datasets/es_OOD_long_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Punjabi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "src=\"en\"\n",
    "dest=\"pa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2769/2769 [08:44<00:00,  5.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2769, 2769)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in tqdm(en_ID):\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  texts.append(translate_text(query, src=src, dest=dest))\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([48, 46, 1, 41, 40],\n",
       " ['ਮੈਨੂੰ ਇਸ ਹਫ਼ਤੇ ਪੰਜ ਵਜੇ ਜਗਾਓ',\n",
       "  'ਸ਼ਾਂਤ',\n",
       "  'ਗੁਲਾਬੀ ਸਿਰਫ ਸਾਨੂੰ ਲੋੜ ਹੈ',\n",
       "  'ਅਤੇ ਹਨੇਰਾ ਡਿੱਗ ਗਿਆ ਹੈ',\n",
       "  'ਓਲੀ ਨੇ ਬੈੱਡਰੂਮ ਦੀਆਂ ਲਾਈਟਾਂ ਬੰਦ ਕਰ ਦਿੱਤੀਆਂ'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2769/2769 [00:00<00:00, 289402.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pa_ID = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "pa_ID = Dataset.from_pandas(pd.DataFrame(data=pa_ID))\n",
    "pa_ID.save_to_disk(\"./datasets/pa_ID_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:37<00:00,  5.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(205, 205)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in tqdm(en_OOD):\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  texts.append(translate_text(query, src=src, dest=dest))\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([56, 57, 57, 57, 56],\n",
       " ['ਕੌਫੀ ਮੇਕਰ ਚਲਾਓ',\n",
       "  'ਗੀਤ ਦਾ ਗਾਇਕ ਕੌਣ ਹੈ',\n",
       "  'ਮੈਂ ਇਸ ਗੀਤ ਬਾਰੇ ਜਾਣਨਾ ਚਾਹੁੰਦਾ ਹਾਂ',\n",
       "  'ਜੋ ਇੱਕ ਲੰਬੀ ਕਾਲੀ ਰੇਲਗੱਡੀ ਬਾਰੇ ਗੀਤ ਗਾਉਂਦਾ ਹੈ',\n",
       "  'ਮੈਨੂੰ ਇੱਕ ਕੌਫੀ ਚਾਹੀਦੀ ਹੈ'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 205/205 [00:00<00:00, 67549.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pa_OOD = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "pa_OOD = Dataset.from_pandas(pd.DataFrame(data=pa_OOD))\n",
    "pa_OOD.save_to_disk(\"./datasets/pa_OOD_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2435/2435 [05:42<00:00,  7.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2435, 2435)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in tqdm(en_ID_short):\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  texts.append(translate_text(query, src=src, dest=dest))\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([48, 46, 1, 41, 40],\n",
       " ['ਮੈਨੂੰ ਇਸ ਹਫ਼ਤੇ ਪੰਜ ਵਜੇ ਜਗਾਓ',\n",
       "  'ਸ਼ਾਂਤ',\n",
       "  'ਗੁਲਾਬੀ ਸਿਰਫ ਸਾਨੂੰ ਲੋੜ ਹੈ',\n",
       "  'ਅਤੇ ਹਨੇਰਾ ਡਿੱਗ ਗਿਆ ਹੈ',\n",
       "  'ਓਲੀ ਨੇ ਬੈੱਡਰੂਮ ਦੀਆਂ ਲਾਈਟਾਂ ਬੰਦ ਕਰ ਦਿੱਤੀਆਂ'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2435/2435 [00:00<00:00, 303926.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pa_ID_short = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "pa_ID_short = Dataset.from_pandas(pd.DataFrame(data=pa_ID_short))\n",
    "pa_ID_short.save_to_disk(\"./datasets/pa_ID_short_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:27<00:00,  6.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(190, 190)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in tqdm(en_OOD_short):\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  texts.append(translate_text(query, src=src, dest=dest))\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([56, 57, 57, 57, 56],\n",
       " ['ਕੌਫੀ ਮੇਕਰ ਚਲਾਓ',\n",
       "  'ਗੀਤ ਦਾ ਗਾਇਕ ਕੌਣ ਹੈ',\n",
       "  'ਮੈਂ ਇਸ ਗੀਤ ਬਾਰੇ ਜਾਣਨਾ ਚਾਹੁੰਦਾ ਹਾਂ',\n",
       "  'ਜੋ ਇੱਕ ਲੰਬੀ ਕਾਲੀ ਰੇਲਗੱਡੀ ਬਾਰੇ ਗੀਤ ਗਾਉਂਦਾ ਹੈ',\n",
       "  'ਮੈਨੂੰ ਇੱਕ ਕੌਫੀ ਚਾਹੀਦੀ ਹੈ'])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 190/190 [00:00<00:00, 47531.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pa_OOD_short = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "pa_OOD_short = Dataset.from_pandas(pd.DataFrame(data=pa_OOD_short))\n",
    "pa_OOD_short.save_to_disk(\"./datasets/pa_OOD_short_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2435/2435 [09:01<00:00,  4.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2435, 2435)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in tqdm(en_ID_long):\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  texts.append(translate_text(query, src=src, dest=dest))\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([48, 46, 1, 41, 40],\n",
       " ['ਮੈਨੂੰ ਇਸ ਹਫਤੇ ਪੰਜ ਵਜੇ ਜਗਾਓ, ਇਸ ਹਫਤੇ ਮੈਨੂੰ ਪੰਜ ਵਜੇ ਜਗਾਓ, ਇਸ ਹਫਤੇ ਮੈਨੂੰ ਪੰਜ ਵਜੇ ਜਗਾਓ, ਇਸ ਹਫਤੇ ਮੈਨੂੰ ਪੰਜ ਵਜੇ ਜਗਾਓ',\n",
       "  'ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ, ਸ਼ਾਂਤ',\n",
       "  'ਗੁਲਾਬੀ ਉਹ ਸਭ ਕੁਝ ਹੈ ਜਿਸਦੀ ਸਾਨੂੰ ਲੋੜ ਹੈ, ਗੁਲਾਬੀ ਉਹ ਸਭ ਕੁਝ ਹੈ ਜਿਸਦੀ ਸਾਨੂੰ ਲੋੜ ਹੈ, ਗੁਲਾਬੀ ਉਹ ਹੈ ਜਿਸਦੀ ਸਾਨੂੰ ਲੋੜ ਹੈ, ਗੁਲਾਬੀ ਉਹ ਸਭ ਕੁਝ ਹੈ ਜਿਸਦੀ ਸਾਨੂੰ ਲੋੜ ਹੈ, ਗੁਲਾਬੀ ਉਹ ਸਭ ਕੁਝ ਹੈ ਜਿਸਦੀ ਸਾਨੂੰ ਲੋੜ ਹੈ',\n",
       "  'ਅਤੇ ਹਨੇਰਾ ਡਿੱਗ ਗਿਆ ਹੈ, ਅਤੇ ਹਨੇਰਾ ਡਿੱਗ ਗਿਆ ਹੈ, ਅਤੇ ਹਨੇਰਾ ਡਿੱਗ ਗਿਆ ਹੈ, ਅਤੇ ਹਨੇਰਾ ਡਿੱਗ ਗਿਆ ਹੈ, ਅਤੇ ਹਨੇਰਾ ਡਿੱਗ ਗਿਆ ਹੈ, ਅਤੇ ਹਨੇਰਾ ਡਿੱਗ ਗਿਆ ਹੈ',\n",
       "  'ਓਲੀ ਬੈੱਡਰੂਮ ਦੀਆਂ ਲਾਈਟਾਂ ਬੰਦ ਕਰ ਦਿਓ, ਓਲੀ ਬੈੱਡਰੂਮ ਦੀਆਂ ਲਾਈਟਾਂ ਬੰਦ ਕਰੋ, ਓਲੀ ਬੈੱਡਰੂਮ ਦੀਆਂ ਲਾਈਟਾਂ ਬੰਦ ਕਰੋ, ਓਲੀ ਬੈੱਡਰੂਮ ਦੀਆਂ ਲਾਈਟਾਂ ਬੰਦ ਕਰੋ'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2435/2435 [00:00<00:00, 303123.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pa_ID_long = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "pa_ID_long = Dataset.from_pandas(pd.DataFrame(data=pa_ID_long))\n",
    "pa_ID_long.save_to_disk(\"./datasets/pa_ID_long_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:43<00:00,  4.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(190, 190)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list()\n",
    "labels = list()\n",
    "\n",
    "for item in tqdm(en_OOD_long):\n",
    "  query = item[\"text\"]\n",
    "  label = item[\"label\"]\n",
    "  texts.append(translate_text(query, src=src, dest=dest))\n",
    "  labels.append(label)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([56, 57, 57, 57, 56],\n",
       " ['ਕੌਫੀ ਮੇਕਰ ਚਲਾਓ, ਕੌਫੀ ਮੇਕਰ ਚਲਾਓ, ਕੌਫੀ ਮੇਕਰ ਚਲਾਓ, ਕੌਫੀ ਮੇਕਰ ਚਲਾਓ, ਕੌਫੀ ਮੇਕਰ ਚਲਾਓ, ਕੌਫੀ ਮੇਕਰ ਚਲਾਓ, ਕੌਫੀ ਮੇਕਰ ਚਲਾਓ, ਕੌਫੀ ਮੇਕਰ ਚਲਾਓ, ਕੌਫੀ ਮੇਕਰ ਚਲਾਓ, ਕੌਫੀ ਮੇਕਰ ਚਲਾਓ',\n",
       "  'ਗੀਤ ਦਾ ਗਾਇਕ ਕੌਣ ਹੈ, ਗੀਤ ਦਾ ਗਾਇਕ ਕੌਣ ਹੈ, ਗੀਤ ਦਾ ਗਾਇਕ ਕੌਣ ਹੈ, ਗੀਤ ਦਾ ਗਾਇਕ ਕੌਣ ਹੈ, ਗੀਤ ਦਾ ਗਾਇਕ ਕੌਣ ਹੈ',\n",
       "  'ਮੈਂ ਇਸ ਗੀਤ ਬਾਰੇ ਜਾਣਨਾ ਚਾਹੁੰਦਾ ਹਾਂ, ਮੈਂ ਇਸ ਗੀਤ ਬਾਰੇ ਜਾਣਨਾ ਚਾਹੁੰਦਾ ਹਾਂ, ਮੈਂ ਇਸ ਗੀਤ ਬਾਰੇ ਜਾਣਨਾ ਚਾਹੁੰਦਾ ਹਾਂ, ਮੈਂ ਇਸ ਗੀਤ ਬਾਰੇ ਜਾਣਨਾ ਚਾਹੁੰਦਾ ਹਾਂ, ਮੈਂ ਇਸ ਗੀਤ ਬਾਰੇ ਜਾਣਨਾ ਚਾਹੁੰਦਾ ਹਾਂ',\n",
       "  'ਕੌਣ ਇੱਕ ਲੰਬੀ ਕਾਲੀ ਰੇਲਗੱਡੀ ਬਾਰੇ ਗੀਤ ਗਾਉਂਦਾ ਹੈ, ਕੌਣ ਇੱਕ ਲੰਬੀ ਕਾਲੀ ਰੇਲਗੱਡੀ ਬਾਰੇ ਗੀਤ ਗਾਉਂਦਾ ਹੈ, ਕੌਣ ਇੱਕ ਲੰਬੀ ਕਾਲੀ ਰੇਲਗੱਡੀ ਬਾਰੇ ਗੀਤ ਗਾਉਂਦਾ ਹੈ, ਕੌਣ ਇੱਕ ਲੰਬੀ ਕਾਲੀ ਰੇਲਗੱਡੀ ਬਾਰੇ ਗੀਤ ਗਾਉਂਦਾ ਹੈ',\n",
       "  'ਮੈਨੂੰ ਕੌਫ਼ੀ ਚਾਹੀਦੀ ਹੈ, ਮੈਨੂੰ ਕੌਫ਼ੀ ਚਾਹੀਦੀ ਹੈ, ਮੈਨੂੰ ਕੌਫ਼ੀ ਚਾਹੀਦੀ ਹੈ, ਮੈਨੂੰ ਕੌਫ਼ੀ ਚਾਹੀਦੀ ਹੈ, ਮੈਨੂੰ ਕੌਫ਼ੀ ਚਾਹੀਦੀ ਹੈ, ਮੈਨੂੰ ਕੌਫ਼ੀ ਚਾਹੀਦੀ ਹੈ, ਮੈਨੂੰ ਕੌਫ਼ੀ ਚਾਹੀਦੀ ਹੈ, ਮੈਨੂੰ ਕੌਫ਼ੀ ਚਾਹੀਦੀ ਹੈ'])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5], texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 190/190 [00:00<00:00, 10322.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pa_OOD_long = {\"label\":labels.copy(), \"text\":texts.copy()}\n",
    "pa_OOD_long = Dataset.from_pandas(pd.DataFrame(data=pa_OOD_long))\n",
    "pa_OOD_long.save_to_disk(\"./datasets/pa_OOD_long_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Save all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,\n",
       " ['en_ID',\n",
       "  'en_ID_short',\n",
       "  'en_ID_long',\n",
       "  'en_OOD',\n",
       "  'en_OOD_short',\n",
       "  'en_OOD_long',\n",
       "  'es_ID',\n",
       "  'es_ID_short',\n",
       "  'es_ID_long',\n",
       "  'es_OOD',\n",
       "  'es_OOD_short',\n",
       "  'es_OOD_long',\n",
       "  'pa_ID',\n",
       "  'pa_ID_short',\n",
       "  'pa_ID_long',\n",
       "  'pa_OOD',\n",
       "  'pa_OOD_short',\n",
       "  'pa_OOD_long'])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = [\"en\", \"es\", \"pa\"]\n",
    "split = [\"ID\", \"OOD\"]\n",
    "versions = [\"\", \"short\", \"long\"]\n",
    "\n",
    "ks = list()\n",
    "\n",
    "for lang in languages:\n",
    "    for s in split:\n",
    "        for v in versions:\n",
    "            set_name = f\"{lang}_{s}\"\n",
    "            if v: set_name += f\"_{v}\"\n",
    "            ks.append(set_name)\n",
    "len(ks), ks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = {\n",
    "    \"en_ID\"         :   en_ID,\n",
    "    \"en_ID_short\"   :   en_ID_short,\n",
    "    \"en_ID_long\"    :   en_ID_long,\n",
    "    \"en_OOD\"        :   en_OOD,\n",
    "    \"en_OOD_short\"  :   en_OOD_short,\n",
    "    \"en_OOD_long\"   :   en_OOD_long,\n",
    "    \"es_ID\"         :   es_ID,\n",
    "    \"es_ID_short\"   :   es_ID_short,\n",
    "    \"es_ID_long\"    :   es_ID_long,\n",
    "    \"es_OOD\"        :   es_OOD,\n",
    "    \"es_OOD_short\"  :   es_OOD_short,\n",
    "    \"es_OOD_long\"   :   es_OOD_long,\n",
    "    \"pa_ID\"         :   pa_ID,\n",
    "    \"pa_ID_short\"   :   pa_ID_short,\n",
    "    \"pa_ID_long\"    :   pa_ID_long,\n",
    "    \"pa_OOD\"        :   pa_OOD,\n",
    "    \"pa_OOD_short\"  :   pa_OOD_short,\n",
    "    \"pa_OOD_long\"   :   pa_OOD_long\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./datasets/all_datasets.pkl\",\"wb\") as f:\n",
    "#     pkl.dump(all_datasets, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"./datasets/all_test_datasets.pkl\", \"rb\") as f:\n",
    "#     pkl_datasets = pkl.load(f)\n",
    "# type(pkl_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = dict()\n",
    "\n",
    "for m in model_names:\n",
    "  preds[m] = dict()\n",
    "  for d_name, data in tqdm(all_datasets.items()):\n",
    "    preds[m][d_name] = models[m][\"classifier\"](data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./predictions/all_test_predictions.pkl\",\"wb\") as f:\n",
    "#     pkl.dump(preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"./predictions/all_test_predictions.pkl\", \"rb\") as f:\n",
    "#     pkl_predictions = pkl.load(f)\n",
    "# type(pkl_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full classification (60 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multi', 'xlm', 'xln']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\n",
    "    \"multi\" :   dict(),\n",
    "    \"xlm\"   :   dict(),\n",
    "    \"xln\"   :   dict()\n",
    "    }\n",
    "\n",
    "model_names = list(models.keys())\n",
    "\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alshouha\\.python_venvs\\transformers\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "models_dir = \"./classifiers/full_en_60\"\n",
    "for m in model_names:\n",
    "  tokenizer_link = f\"{models_dir}/{m}/{m}_tokenizer\"\n",
    "  model_link = f\"{models_dir}/{m}/{m}_model\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(tokenizer_link)\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(model_link)\n",
    "  classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, function_to_apply=\"none\", return_all_scores=True)\n",
    "  models[m][\"tokenizer\"] = tokenizer\n",
    "  models[m][\"model\"] = model\n",
    "  models[m][\"classifier\"] = classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categroies_list = [\n",
    "    'datetime_query',\n",
    "    'iot_hue_lightchange',\n",
    "    'transport_ticket',\n",
    "    'takeaway_query',\n",
    "    'qa_stock',\n",
    "    'general_greet',\n",
    "    'recommendation_events',\n",
    "    'music_dislikeness',\n",
    "    'iot_wemo_off',\n",
    "    'cooking_recipe',\n",
    "    'qa_currency',\n",
    "    'transport_traffic',\n",
    "    'general_quirky',\n",
    "    'weather_query',\n",
    "    'audio_volume_up',\n",
    "    'email_addcontact',\n",
    "    'takeaway_order',\n",
    "    'email_querycontact',\n",
    "    'iot_hue_lightup',\n",
    "    'recommendation_locations',\n",
    "    'play_audiobook',\n",
    "    'lists_createoradd',\n",
    "    'news_query',\n",
    "    'alarm_query',\n",
    "    'iot_wemo_on',\n",
    "    'general_joke',\n",
    "    'qa_definition',\n",
    "    'social_query',\n",
    "    'music_settings',\n",
    "    'audio_volume_other',\n",
    "    'calendar_remove',\n",
    "    'iot_hue_lightdim',\n",
    "    'calendar_query',\n",
    "    'email_sendemail',\n",
    "    'iot_cleaning',\n",
    "    'audio_volume_down',\n",
    "    'play_radio',\n",
    "    'cooking_query',\n",
    "    'datetime_convert',\n",
    "    'qa_maths',\n",
    "    'iot_hue_lightoff',\n",
    "    'iot_hue_lighton',\n",
    "    'transport_query',\n",
    "    'music_likeness',\n",
    "    'email_query',\n",
    "    'play_music',\n",
    "    'audio_volume_mute',\n",
    "    'social_post',\n",
    "    'alarm_set',\n",
    "    'qa_factoid',\n",
    "    'calendar_set',\n",
    "    'play_game',\n",
    "    'alarm_remove',\n",
    "    'lists_remove',\n",
    "    'transport_taxi',\n",
    "    'recommendation_movies',\n",
    "    'iot_coffee',\n",
    "    'music_query',\n",
    "    'play_podcasts',\n",
    "    'lists_query']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Massive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_versions = [\"train\", \"valid\", \"test\", \"train_valid\"]\n",
    "en_datasets = dict()\n",
    "\n",
    "folder_path = \"datasets/full_en_eo_pa\"\n",
    "\n",
    "ds_name = \"en\"\n",
    "t = \"short\"\n",
    "for v in datasets_versions:\n",
    "    en_datasets[v] = load_from_disk(f\"{folder_path}/{ds_name}_{v}_{t}.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 11514\n",
       " }),\n",
       " 'valid': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2033\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2974\n",
       " }),\n",
       " 'train_valid': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 13547\n",
       " })}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "        51, 52, 53, 54, 55, 56, 57, 58, 59]),\n",
       " array([414, 147, 152, 146, 176,  27, 216,  16,  57, 248, 174, 139, 660,\n",
       "        699, 122,  59, 155, 143,  88, 204, 185, 202, 585, 149,  55,  87,\n",
       "        322, 126,  59,  18, 359,  93, 668, 417, 112,  60, 329,   6,  61,\n",
       "         91, 170,  27, 263, 129, 491, 762, 125, 333, 213, 634, 941, 134,\n",
       "         92, 201, 127,  82, 138, 184, 227, 248], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list, counts = np.unique(en_datasets[\"train_valid\"][\"label\"], return_counts=True)\n",
    "labels_list, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(labels_list)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English (en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_len_thr = 512\n",
    "long_query = query_len_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'valid', 'test']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_versions[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start > train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:55<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: len of ds > 11514\n",
      "train: len of train ds > 11514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 11514/11514 [00:00<00:00, 564687.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created > train\n",
      "Start > valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:08<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: len of ds > 2033\n",
      "valid: len of valid ds > 2033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2033/2033 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created > valid\n",
      "Start > test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:13<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: len of ds > 2974\n",
      "test: len of test ds > 2974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2974/2974 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created > test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds_name = \"en\"\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "### loading a tokenizer for the workaround; e,g, multi\n",
    "m = \"multi\"\n",
    "tokenizer_link = f\"{models_dir}/{m}/{m}_tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_link)\n",
    "\n",
    "for v in datasets_versions[:-1]:\n",
    "    print(f\"Start > {v}\")\n",
    "    ds = en_datasets[v]\n",
    "    ds_list = list()\n",
    "    texts = list()\n",
    "    labels = list()\n",
    "    combined_ds = None\n",
    "    index = 0\n",
    "    batches = math.ceil(ds.num_rows/batch_size)\n",
    "    for i in tqdm(range(batches)):\n",
    "        last_index = index+batch_size\n",
    "        if last_index>=len(ds): last_index=None\n",
    "        texts_batch = ds[\"text\"][index:last_index]\n",
    "        labels_batch = ds[\"label\"][index:last_index]\n",
    "        index = (i+1)*batch_size\n",
    "        ds_list = list()\n",
    "        for text, label in zip(texts_batch, labels_batch):\n",
    "            # query = item[\"text\"]\n",
    "            # label = item[\"label\"]\n",
    "            query = text\n",
    "            query_len = len(query.split())\n",
    "            repeat = math.floor(long_query/query_len)\n",
    "            query = [query for _ in range(repeat)]\n",
    "            query = \", \".join(query)\n",
    "            ##########################################\n",
    "            # This part is a workaround to avoid longer tokenized sequences, truncation and model_max_length were useless when loading the tokenizer\n",
    "            query_len = len(query.split())\n",
    "            t_query = tokenizer(query, return_tensors=\"pt\")\n",
    "            tokenized_len = len(t_query[0])\n",
    "            red_perc = (tokenized_len-long_query)/tokenized_len\n",
    "            num_red_tokens = int(red_perc*query_len)+5 # +5 to make sure that it is less than 512\n",
    "            query = \" \".join(query.split()[:-num_red_tokens])\n",
    "            new_len = len(tokenizer(query, return_tensors=\"pt\")[0])\n",
    "            # if (new_len>long_query): print(f\"Error: {new_len}\") # check\n",
    "            ##########################################\n",
    "            # texts.append(query)\n",
    "            # labels.append(label)\n",
    "            ds_list.append({\"label\": label, \"text\":query})\n",
    "\n",
    "        batch_ds = Dataset.from_list(ds_list)\n",
    "\n",
    "        if not combined_ds:\n",
    "            combined_ds = batch_ds\n",
    "        else:\n",
    "            combined_ds = concatenate_datasets([combined_ds, batch_ds])\n",
    "\n",
    "    \n",
    "    print(f\"{v}: len of ds > {ds.num_rows}\")\n",
    "    print(f\"{v}: len of {v} ds > {combined_ds.num_rows}\")\n",
    "    # print(f\"example len of instance: {len(ds_list[1][\"text\"].split())}\")\n",
    "\n",
    "    combined_ds.save_to_disk(f\"{folder_path}/{ds_name}_{v}_long.hf\")\n",
    "\n",
    "    print(f\"Created > {v}\")\n",
    "    del texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 11514\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2033\n",
       " }))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = Dataset.load_from_disk(f\"{folder_path}/{ds_name}_train_long.hf\")\n",
    "valid = Dataset.load_from_disk(f\"{folder_path}/{ds_name}_valid_long.hf\")\n",
    "\n",
    "train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 13547\n",
       "})"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ds = concatenate_datasets([train, valid])\n",
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 13547/13547 [00:00<00:00, 292041.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "combined_ds.save_to_disk(f\"{folder_path}/{ds_name}_train_valid_long.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_long': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 11514\n",
       " }),\n",
       " 'valid_long': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2033\n",
       " }),\n",
       " 'test_long': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2974\n",
       " }),\n",
       " 'train_valid_long': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 13547\n",
       " })}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_name = \"en\"\n",
    "t = \"long\"\n",
    "\n",
    "en_datasets_long = {f\"{v}_long\":Dataset.load_from_disk(f\"{folder_path}/{ds_name}_{v}_{t}.hf\") for v in datasets_versions}\n",
    "\n",
    "en_datasets_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esperanto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_name = \"eo\" #Esperanto\n",
    "\n",
    "# batch_size = 100\n",
    "\n",
    "# for v in datasets_versions[1:-1]:\n",
    "#     print(f\"Start > {v}\")\n",
    "#     ds = en_datasets[v]\n",
    "#     ds_list = list()\n",
    "#     texts = list()\n",
    "#     labels = list()\n",
    "#     combined_ds = None\n",
    "#     index = 0\n",
    "#     batches = math.ceil(ds.num_rows/batch_size)\n",
    "#     for i in tqdm(range(batches)):\n",
    "#         last_index = index+batch_size\n",
    "#         if last_index>=len(ds): last_index=None\n",
    "#         texts_batch = ds[\"text\"][index:last_index]\n",
    "#         labels_batch = ds[\"label\"][index:last_index]\n",
    "#         index = (i+1)*batch_size\n",
    "#         ds_list = list()\n",
    "#         for text, label in zip(texts_batch, labels_batch):\n",
    "#             # query = item[\"text\"]\n",
    "#             # label = item[\"label\"]\n",
    "#             query = translate_text(text, src=\"en\", dest=ds_name)\n",
    "#             # texts.append(query)\n",
    "#             # labels.append(label)\n",
    "#             ds_list.append({\"label\": label, \"text\":query})\n",
    "\n",
    "#         batch_ds = Dataset.from_list(ds_list)\n",
    "\n",
    "#         if not combined_ds:\n",
    "#             combined_ds = batch_ds\n",
    "#         else:\n",
    "#             combined_ds = concatenate_datasets([combined_ds, batch_ds])\n",
    "\n",
    "    \n",
    "#     print(f\"{v}: len of ds > {ds.num_rows}\")\n",
    "#     print(f\"{v}: len of {ds_name} ds > {combined_ds.num_rows}\")\n",
    "#     # print(f\"example len of instance: {len(ds_list[1][\"text\"].split())}\")\n",
    "\n",
    "#     combined_ds.save_to_disk(f\"{folder_path}/{ds_name}_{v}.hf\")\n",
    "\n",
    "#     print(f\"Created > {v}\")\n",
    "#     del texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eo_train = Dataset.load_from_disk(f\"{folder_path}/{ds_name}_train.hf\")\n",
    "# eo_valid = Dataset.load_from_disk(f\"{folder_path}/{ds_name}_valid.hf\")\n",
    "# eo_test = Dataset.load_from_disk(f\"{folder_path}/{ds_name}_test.hf\")\n",
    "# print(eo_train, eo_valid, eo_test)\n",
    "\n",
    "# eo_combined_ds = concatenate_datasets([eo_train, eo_valid])\n",
    "# print(eo_combined_ds)\n",
    "\n",
    "# eo_combined_ds.save_to_disk(f\"{folder_path}/{ds_name}_train_valid.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_versions = [\"train\", \"valid\", \"test\", \"train_valid\"]\n",
    "eo_datasets = dict()\n",
    "\n",
    "folder_path = \"datasets/full_en_eo_pa\"\n",
    "\n",
    "ds_name = \"eo\"\n",
    "t = \"short\"\n",
    "\n",
    "for v in datasets_versions:\n",
    "    eo_datasets[v] = load_from_disk(f\"{folder_path}/{ds_name}_{v}_{t}.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 11514\n",
       " }),\n",
       " 'valid': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2033\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2974\n",
       " }),\n",
       " 'train_valid': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 13547\n",
       " })}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eo_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_name = \"eo\" #Esperanto\n",
    "\n",
    "# batch_size = 100\n",
    "\n",
    "# for v in datasets_versions[:1]: # Only first item for now\n",
    "#     print(f\"Start > {v}\")\n",
    "#     ds = en_datasets[v]\n",
    "#     ds_list = list()\n",
    "#     texts = list()\n",
    "#     labels = list()\n",
    "#     combined_ds = None\n",
    "#     index = 0\n",
    "#     batches = math.ceil(ds.num_rows/batch_size)\n",
    "#     for i in tqdm(range(batches)):\n",
    "#         last_index = index+batch_size\n",
    "#         if last_index>=len(ds): last_index=None\n",
    "#         texts_batch = ds[\"text\"][index:last_index]\n",
    "#         labels_batch = ds[\"label\"][index:last_index]\n",
    "#         ds_list = list()\n",
    "#         for text, label in zip(texts_batch, labels_batch):\n",
    "#             # query = item[\"text\"]\n",
    "#             # label = item[\"label\"]\n",
    "#             query = translate_text(text, src=\"en\", dest=ds_name)\n",
    "#             # texts.append(query)\n",
    "#             # labels.append(label)\n",
    "#             ds_list.append({\"label\": label, \"text\":query})\n",
    "\n",
    "#         batch_ds = Dataset.from_list(ds_list)\n",
    "\n",
    "#         batch_ds.save_to_disk(f\"{folder_path}/{ds_name}_{v}_batches/full_{ds_name}_{v}_{index}.hf\")    \n",
    "#         print(f\"{v}: len of ds {index}> {ds.num_rows}\")\n",
    "#         print(f\"{v}: len of eo ds {index}> {batch_ds.num_rows}\")\n",
    "        \n",
    "#         index = (i+1)*batch_size\n",
    "\n",
    "#         # if not combined_ds:\n",
    "#         #     combined_ds = batch_ds\n",
    "#         # else:\n",
    "#         #     combined_ds = concatenate_datasets([combined_ds, batch_ds])\n",
    "\n",
    "    \n",
    "#         # print(f\"{v}: len of ds {index}> {ds.num_rows}\")\n",
    "#         # print(f\"{v}: len of eo ds {index}> {combined_ds.num_rows}\")\n",
    "#     # print(f\"example len of instance: {len(ds_list[1][\"text\"].split())}\")\n",
    "\n",
    "#     # combined_ds.save_to_disk(f\"{folder_path}/full_{ds_name}_{v}.hf\")\n",
    "\n",
    "#     print(f\"Created > {v}\")\n",
    "#     del texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start > train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:36<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: len of ds > 11514\n",
      "train: len of train ds > 11514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 11514/11514 [00:00<00:00, 681770.54 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created > train\n",
      "Start > valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:06<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: len of ds > 2033\n",
      "valid: len of valid ds > 2033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2033/2033 [00:00<00:00, 199831.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created > valid\n",
      "Start > test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: len of ds > 2974\n",
      "test: len of test ds > 2974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2974/2974 [00:00<00:00, 369103.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created > test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query_len_thr = 512\n",
    "long_query = query_len_thr\n",
    "\n",
    "ds_name = \"eo\"\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "### loading a tokenizer for the workaround; e,g, multi\n",
    "m = \"multi\"\n",
    "tokenizer_link = f\"{models_dir}/{m}/{m}_tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_link)\n",
    "\n",
    "for v in datasets_versions[:-1]:\n",
    "    print(f\"Start > {v}\")\n",
    "    ds = eo_datasets[v]\n",
    "    ds_list = list()\n",
    "    texts = list()\n",
    "    labels = list()\n",
    "    combined_ds = None\n",
    "    index = 0\n",
    "    batches = math.ceil(ds.num_rows/batch_size)\n",
    "    for i in tqdm(range(batches)):\n",
    "        last_index = index+batch_size\n",
    "        if last_index>=len(ds): last_index=None\n",
    "        texts_batch = ds[\"text\"][index:last_index]\n",
    "        labels_batch = ds[\"label\"][index:last_index]\n",
    "        index = (i+1)*batch_size\n",
    "        ds_list = list()\n",
    "        for text, label in zip(texts_batch, labels_batch):\n",
    "            # query = item[\"text\"]\n",
    "            # label = item[\"label\"]\n",
    "            query = text\n",
    "            query_len = len(query.split())\n",
    "            repeat = math.ceil(long_query/query_len)\n",
    "            query = [query for _ in range(repeat)]\n",
    "            query = \", \".join(query)\n",
    "            ##########################################\n",
    "            # This part is a workaround to avoid longer tokenized sequences, truncation and model_max_length were useless when loading the tokenizer\n",
    "            query_len = len(query.split())\n",
    "            t_query = tokenizer(query, return_tensors=\"pt\")\n",
    "            tokenized_len = len(t_query[0])\n",
    "            red_perc = (tokenized_len-long_query)/tokenized_len\n",
    "            num_red_tokens = int(red_perc*query_len)+5 # +5 to make sure that it is less than 512\n",
    "            query = \" \".join(query.split()[:-num_red_tokens])\n",
    "            new_len = len(tokenizer(query, return_tensors=\"pt\"))\n",
    "            # if (new_len>long_query): print(f\"Error: {new_len}\") # check\n",
    "            ##########################################\n",
    "            # texts.append(query)\n",
    "            # labels.append(label)\n",
    "            ds_list.append({\"label\": label, \"text\":query})\n",
    "\n",
    "        batch_ds = Dataset.from_list(ds_list)\n",
    "\n",
    "        if not combined_ds:\n",
    "            combined_ds = batch_ds\n",
    "        else:\n",
    "            combined_ds = concatenate_datasets([combined_ds, batch_ds])\n",
    "\n",
    "    \n",
    "    print(f\"{v}: len of ds > {ds.num_rows}\")\n",
    "    print(f\"{v}: len of {v} ds > {combined_ds.num_rows}\")\n",
    "    # print(f\"example len of instance: {len(ds_list[1][\"text\"].split())}\")\n",
    "\n",
    "    combined_ds.save_to_disk(f\"{folder_path}/{ds_name}_{v}_long.hf\")\n",
    "\n",
    "    print(f\"Created > {v}\")\n",
    "    del texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 11514\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2033\n",
       " }))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = Dataset.load_from_disk(f\"{folder_path}/{ds_name}_train_long.hf\")\n",
    "valid = Dataset.load_from_disk(f\"{folder_path}/{ds_name}_valid_long.hf\")\n",
    "\n",
    "train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 13547\n",
       "})"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ds = concatenate_datasets([train, valid])\n",
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 13547/13547 [00:00<00:00, 379449.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "combined_ds.save_to_disk(f\"{folder_path}/{ds_name}_train_valid_long.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_long': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 11514\n",
       " }),\n",
       " 'valid_long': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2033\n",
       " }),\n",
       " 'test_long': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2974\n",
       " }),\n",
       " 'train_valid_long': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 13547\n",
       " })}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_name = \"eo\"\n",
    "t = \"long\"\n",
    "\n",
    "eo_datasets_long = {f\"{v}_long\":Dataset.load_from_disk(f\"{folder_path}/{ds_name}_{v}_{t}.hf\") for v in datasets_versions}\n",
    "\n",
    "eo_datasets_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punjabi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_name = \"pa\" #Punjabi\n",
    "\n",
    "# batch_size = 100\n",
    "\n",
    "# for v in datasets_versions[:1]: # Only first item for now (TRAIN)\n",
    "#     print(f\"Start > {v}\")\n",
    "#     ds = en_datasets[v]\n",
    "#     ds_list = list()\n",
    "#     texts = list()\n",
    "#     labels = list()\n",
    "#     combined_ds = None\n",
    "#     i = 0 + 7700\n",
    "#     batches = math.ceil(ds.num_rows/batch_size)\n",
    "#     while tqdm(i<ds.num_rows): # tqdm(range(batches)):\n",
    "#         last_index = i+batch_size\n",
    "#         if last_index>=len(ds): last_index=None\n",
    "#         texts_batch = ds[\"text\"][i:last_index]\n",
    "#         labels_batch = ds[\"label\"][i:last_index]\n",
    "#         ds_list = list()\n",
    "#         for text, label in zip(texts_batch, labels_batch):\n",
    "#             # query = item[\"text\"]\n",
    "#             # label = item[\"label\"]\n",
    "#             query = translate_text(text, src=\"en\", dest=ds_name)\n",
    "#             # texts.append(query)\n",
    "#             # labels.append(label)\n",
    "#             ds_list.append({\"label\": label, \"text\":query})\n",
    "\n",
    "#         batch_ds = Dataset.from_list(ds_list)\n",
    "\n",
    "#         batch_ds.save_to_disk(f\"{folder_path}/{ds_name}_{v}_batches/_{ds_name}_{v}_{i}.hf\")\n",
    "#         print(f\"{i} of {ds.num_rows} ds are saved!\")    \n",
    "#         # print(f\"{v}: len of ds {index}> {ds.num_rows}\")\n",
    "#         # print(f\"{v}: len of {ds_name} ds {index}> {batch_ds.num_rows}\")\n",
    "        \n",
    "#         i += batch_size\n",
    "\n",
    "#         # if not combined_ds:\n",
    "#         #     combined_ds = batch_ds\n",
    "#         # else:\n",
    "#         #     combined_ds = concatenate_datasets([combined_ds, batch_ds])\n",
    "\n",
    "    \n",
    "#         # print(f\"{v}: len of ds {index}> {ds.num_rows}\")\n",
    "#         # print(f\"{v}: len of eo ds {index}> {combined_ds.num_rows}\")\n",
    "#     # print(f\"example len of instance: {len(ds_list[1][\"text\"].split())}\")\n",
    "\n",
    "#     # combined_ds.save_to_disk(f\"{folder_path}/full_{ds_name}_{v}.hf\")\n",
    "\n",
    "#     print(f\"Created > {v}\")\n",
    "#     del texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_name = \"pa\" #Punjabi\n",
    "# batch_size = 100\n",
    "\n",
    "# v = \"train\"\n",
    "# batches_path = f\"{folder_path}/{ds_name}_{v}_batches\"\n",
    "\n",
    "# combined_ds = None\n",
    "\n",
    "# for i in tqdm(range(0, en_datasets[v].num_rows, batch_size)):\n",
    "#     ds_path = f\"{batches_path}/_{ds_name}_{v}_{i}.hf\"\n",
    "#     batch_ds = Dataset.load_from_disk(f\"{ds_path}\")\n",
    "\n",
    "#     if not combined_ds:\n",
    "#         combined_ds = batch_ds\n",
    "#     else:\n",
    "#         combined_ds = concatenate_datasets([combined_ds, batch_ds])\n",
    "\n",
    "# print(combined_ds.num_rows)\n",
    "# combined_ds.save_to_disk(f\"{folder_path}/{ds_name}_{v}.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_name = \"pa\" #Punjabi\n",
    "\n",
    "# batch_size = 100\n",
    "\n",
    "# for v in datasets_versions[:1]:\n",
    "#     print(f\"Start > {v}\")\n",
    "#     ds = en_datasets[v]\n",
    "#     ds_list = list()\n",
    "#     texts = list()\n",
    "#     labels = list()\n",
    "#     combined_ds = None\n",
    "#     index = 0\n",
    "#     batches = math.ceil(ds.num_rows/batch_size)\n",
    "#     for i in tqdm(range(batches)):\n",
    "#         last_index = index+batch_size\n",
    "#         if last_index>=len(ds): last_index=None\n",
    "#         texts_batch = ds[\"text\"][index:last_index]\n",
    "#         labels_batch = ds[\"label\"][index:last_index]\n",
    "#         index = (i+1)*batch_size\n",
    "#         ds_list = list()\n",
    "#         for text, label in zip(texts_batch, labels_batch):\n",
    "#             # query = item[\"text\"]\n",
    "#             # label = item[\"label\"]\n",
    "#             query = translate_text(text, src=\"en\", dest=ds_name)\n",
    "#             # texts.append(query)\n",
    "#             # labels.append(label)\n",
    "#             ds_list.append({\"label\": label, \"text\":query})\n",
    "\n",
    "#         batch_ds = Dataset.from_list(ds_list)\n",
    "\n",
    "#         if not combined_ds:\n",
    "#             combined_ds = batch_ds\n",
    "#         else:\n",
    "#             combined_ds = concatenate_datasets([combined_ds, batch_ds])\n",
    "\n",
    "    \n",
    "#     print(f\"{v}: len of ds > {ds.num_rows}\")\n",
    "#     print(f\"{v}: len of {ds_name} ds > {combined_ds.num_rows}\")\n",
    "#     # print(f\"example len of instance: {len(ds_list[1][\"text\"].split())}\")\n",
    "\n",
    "#     combined_ds.save_to_disk(f\"{folder_path}/{ds_name}_{v}.hf\")\n",
    "\n",
    "#     print(f\"Created > {v}\")\n",
    "#     del texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'text'],\n",
      "    num_rows: 11514\n",
      "}) Dataset({\n",
      "    features: ['label', 'text'],\n",
      "    num_rows: 2033\n",
      "}) Dataset({\n",
      "    features: ['label', 'text'],\n",
      "    num_rows: 2974\n",
      "})\n",
      "Dataset({\n",
      "    features: ['label', 'text'],\n",
      "    num_rows: 13547\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 13547/13547 [00:00<00:00, 1231261.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pa_train = Dataset.load_from_disk(f\"{folder_path}/{ds_name}_train.hf\")\n",
    "pa_valid = Dataset.load_from_disk(f\"{folder_path}/{ds_name}_valid.hf\")\n",
    "pa_test = Dataset.load_from_disk(f\"{folder_path}/{ds_name}_test.hf\")\n",
    "print(pa_train, pa_valid, pa_test)\n",
    "\n",
    "pa_combined_ds = concatenate_datasets([pa_train, pa_valid])\n",
    "print(pa_combined_ds)\n",
    "\n",
    "pa_combined_ds.save_to_disk(f\"{folder_path}/{ds_name}_train_valid.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_versions = [\"train\", \"valid\", \"test\", \"train_valid\"]\n",
    "pa_datasets = dict()\n",
    "\n",
    "folder_path = \"datasets/full_en_eo_pa\"\n",
    "\n",
    "ds_name = \"pa\"\n",
    "t = \"short\"\n",
    "\n",
    "for v in datasets_versions:\n",
    "    pa_datasets[v] = load_from_disk(f\"{folder_path}/{ds_name}_{v}_{t}.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 11514\n",
       " }),\n",
       " 'valid': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2033\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2974\n",
       " }),\n",
       " 'train_valid': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 13547\n",
       " })}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start > train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:54<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: len of ds > 11514\n",
      "train: len of train ds > 11514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 11514/11514 [00:00<00:00, 254046.46 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created > train\n",
      "Start > valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:13<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: len of ds > 2033\n",
      "valid: len of valid ds > 2033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2033/2033 [00:00<00:00, 100619.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created > valid\n",
      "Start > test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: len of ds > 2974\n",
      "test: len of test ds > 2974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2974/2974 [00:00<00:00, 205757.79 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created > test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query_len_thr = 512\n",
    "long_query = query_len_thr\n",
    "\n",
    "ds_name = \"pa\"\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "### loading a tokenizer for the workaround; e,g, multi\n",
    "m = \"multi\"\n",
    "tokenizer_link = f\"{models_dir}/{m}/{m}_tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_link)\n",
    "\n",
    "for v in datasets_versions[:-1]:\n",
    "    print(f\"Start > {v}\")\n",
    "    ds = pa_datasets[v]\n",
    "    ds_list = list()\n",
    "    texts = list()\n",
    "    labels = list()\n",
    "    combined_ds = None\n",
    "    index = 0\n",
    "    batches = math.ceil(ds.num_rows/batch_size)\n",
    "    for i in tqdm(range(batches)):\n",
    "        last_index = index+batch_size\n",
    "        if last_index>=len(ds): last_index=None\n",
    "        texts_batch = ds[\"text\"][index:last_index]\n",
    "        labels_batch = ds[\"label\"][index:last_index]\n",
    "        index = (i+1)*batch_size\n",
    "        ds_list = list()\n",
    "        for text, label in zip(texts_batch, labels_batch):\n",
    "            # query = item[\"text\"]\n",
    "            # label = item[\"label\"]\n",
    "            query = text\n",
    "            query_len = len(query.split())\n",
    "            repeat = math.ceil(long_query/query_len)\n",
    "            query = [query for _ in range(repeat)]\n",
    "            query = \", \".join(query)\n",
    "            ##########################################\n",
    "            # This part is a workaround to avoid longer tokenized sequences, truncation and model_max_length were useless when loading the tokenizer\n",
    "            query_len = len(query.split())\n",
    "            t_query = tokenizer(query, return_tensors=\"pt\")\n",
    "            tokenized_len = len(t_query[0])\n",
    "            red_perc = (tokenized_len-long_query)/tokenized_len\n",
    "            num_red_tokens = int(red_perc*query_len)+5 # +5 to make sure that it is less than 512\n",
    "            query = \" \".join(query.split()[:-num_red_tokens])\n",
    "            new_len = len(tokenizer(query, return_tensors=\"pt\"))\n",
    "            # if (new_len>long_query): print(f\"Error: {new_len}\") # check\n",
    "            ##########################################\n",
    "            # texts.append(query)\n",
    "            # labels.append(label)\n",
    "            ds_list.append({\"label\": label, \"text\":query})\n",
    "\n",
    "        batch_ds = Dataset.from_list(ds_list)\n",
    "\n",
    "        if not combined_ds:\n",
    "            combined_ds = batch_ds\n",
    "        else:\n",
    "            combined_ds = concatenate_datasets([combined_ds, batch_ds])\n",
    "\n",
    "    \n",
    "    print(f\"{v}: len of ds > {ds.num_rows}\")\n",
    "    print(f\"{v}: len of {v} ds > {combined_ds.num_rows}\")\n",
    "    # print(f\"example len of instance: {len(ds_list[1][\"text\"].split())}\")\n",
    "\n",
    "    combined_ds.save_to_disk(f\"{folder_path}/{ds_name}_{v}_long.hf\")\n",
    "\n",
    "    print(f\"Created > {v}\")\n",
    "    del texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 11514\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2033\n",
       " }))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = Dataset.load_from_disk(f\"{folder_path}/{ds_name}_train_long.hf\")\n",
    "valid = Dataset.load_from_disk(f\"{folder_path}/{ds_name}_valid_long.hf\")\n",
    "\n",
    "train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 13547\n",
       "})"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ds = concatenate_datasets([train, valid])\n",
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 13547/13547 [00:00<00:00, 309353.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "combined_ds.save_to_disk(f\"{folder_path}/{ds_name}_train_valid_long.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_long': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 11514\n",
       " }),\n",
       " 'valid_long': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2033\n",
       " }),\n",
       " 'test_long': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 2974\n",
       " }),\n",
       " 'train_valid_long': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 13547\n",
       " })}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_name = \"pa\"\n",
    "t = \"long\"\n",
    "\n",
    "pa_datasets_long = {f\"{v}_long\":Dataset.load_from_disk(f\"{folder_path}/{ds_name}_{v}_{t}.hf\") for v in datasets_versions}\n",
    "\n",
    "pa_datasets_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names = [\"en\", \"eo\", \"pa\"]\n",
    "datasets_types = [\"short\", \"long\"]\n",
    "datasets_versions = [\"train\", \"valid\", \"test\", \"train_valid\"]\n",
    "\n",
    "datasets_main_dict = {\n",
    "    \"en\"    :   {\n",
    "        \"short\" : en_datasets,\n",
    "        \"long\"  : en_datasets_long\n",
    "    },\n",
    "    \"eo\"    :   {\n",
    "        \"short\" : eo_datasets,\n",
    "        \"long\"  : eo_datasets_long\n",
    "    },\n",
    "    \"pa\"    :   {\n",
    "        \"short\" : pa_datasets,\n",
    "        \"long\"  : pa_datasets_long\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'valid', 'test']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_versions[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = dict()\n",
    "\n",
    "for n in datasets_names:\n",
    "    for t in datasets_types:\n",
    "        for v in datasets_versions[:-1]:\n",
    "            key = v\n",
    "            if t==\"long\": key+=\"_long\" \n",
    "            all_datasets[f\"{n}_{v}_{t}\"] = datasets_main_dict[n][t][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,\n",
       " {'en_train_short': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 11514\n",
       "  }),\n",
       "  'en_valid_short': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 2033\n",
       "  }),\n",
       "  'en_test_short': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 2974\n",
       "  }),\n",
       "  'en_train_long': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 11514\n",
       "  }),\n",
       "  'en_valid_long': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 2033\n",
       "  }),\n",
       "  'en_test_long': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 2974\n",
       "  }),\n",
       "  'eo_train_short': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 11514\n",
       "  }),\n",
       "  'eo_valid_short': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 2033\n",
       "  }),\n",
       "  'eo_test_short': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 2974\n",
       "  }),\n",
       "  'eo_train_long': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 11514\n",
       "  }),\n",
       "  'eo_valid_long': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 2033\n",
       "  }),\n",
       "  'eo_test_long': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 2974\n",
       "  }),\n",
       "  'pa_train_short': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 11514\n",
       "  }),\n",
       "  'pa_valid_short': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 2033\n",
       "  }),\n",
       "  'pa_test_short': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 2974\n",
       "  }),\n",
       "  'pa_train_long': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 11514\n",
       "  }),\n",
       "  'pa_valid_long': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 2033\n",
       "  }),\n",
       "  'pa_test_long': Dataset({\n",
       "      features: ['label', 'text'],\n",
       "      num_rows: 2974\n",
       "  })})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_datasets), all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multi', 'xlm', 'xln']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> xlm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\teo_test_long starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [41:54<00:00, 139.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\teo_test_long is dumped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_path = \"predictions/full_en_eo_pa/separate\"\n",
    "\n",
    "preds = dict()\n",
    "\n",
    "for m in model_names[1:2]:\n",
    "  print(f\">>>> {m}\")\n",
    "  preds[m] = dict()\n",
    "  for d_name, data in tqdm(all_datasets.items()):\n",
    "    if (\"eo_test_long\" == d_name):\n",
    "      pred = dict()\n",
    "      print(f\"\\t{d_name} starts...\")\n",
    "      pred[f\"{m}_{d_name}\"] = models[m][\"classifier\"](data[\"text\"])\n",
    "      with open(f\"{pred_path}/{m}_{d_name}_preds.pkl\", \"wb\") as f:\n",
    "        pkl.dump(pred, f)\n",
    "      print(f\"\\t{d_name} is dumped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Merge train and Valid after finishing\n",
    "####### becareful \"multi_en_train_valid_short_preds\" is already done!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = dict()\n",
    "\n",
    "pred_path = \"predictions/full_en_eo_pa\"\n",
    "\n",
    "for m in model:\n",
    "    for n in datasets_names:\n",
    "        for t in datasets_types:\n",
    "            for v in datasets_versions:\n",
    "                ds_pkl_path = f\"{pred_path}/separate/{m}_{n}_{v}_{t}_preds.pkl\"\n",
    "                with open(ds_pkl_path, \"rb\") as f:\n",
    "                    ds_pkl = pkl.load(f)\n",
    "                    all_preds.update(ds_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"{pred_path}/all_predictions.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with open(f\"{pred_path}/all_predictions.pkl\", \"rb\") as f:\n",
    "#     pkl_predictions = pkl.load(f)\n",
    "# type(pkl_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
